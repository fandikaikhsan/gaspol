# Admin Setup Checklist

This document lists all steps needed for the baseline assessment to work correctly.

## Prerequisites

1. Access to Supabase Dashboard > SQL Editor
2. Admin role in the database
3. Anthropic API key configured in environment

## Setup Flow

### Step 1: Run Database Migrations

Apply these in Supabase SQL Editor:

```sql
-- Run in order:
-- 1. scripts/fix-analytics-schema.sql
-- 2. scripts/comprehensive-rls-policies.sql (optional but recommended)
```

### Step 2: Run Exam Research (Critical)

The exam research process generates exam-specific data including:
- **Batch 1**: Exam structure, timing, scoring
- **Batch 2**: Content taxonomy, topics, skills
- **Batch 3**: Construct profiling per taxonomy node
- **Batch 4**: Error pattern profiling (NEW)

**To run research:**
1. Go to Admin > Exam Research
2. Enter exam type (e.g., "UTBK-SNBT") and year
3. Click "Run Research"
4. Wait for all 4 batches to complete

This will:
- Create exam-specific error tags (replacing generic ones)
- Set construct weights for taxonomy nodes
- Populate question generation guidance

### Step 3: (Optional) Bootstrap Error Tags

If you need error tagging BEFORE running research:

**File:** `scripts/seed-error-tags.sql`
**Purpose:** Creates generic fallback error tags. These will be replaced/extended when you run research.

### Step 4: Diagnose Question Issues

**File:** `scripts/diagnose-and-fix-questions.sql`
**Purpose:** Diagnoses and fixes question format issues.

## Question Format Requirements

For questions to render correctly in the UI, they must have:

### Required Fields:
- `stem` (TEXT): The question text displayed to the user
- `options` (JSONB): Object with keys A, B, C, D, E containing option text
- `correct_answer` (TEXT): The correct option key (e.g., "D")
- `question_format` (TEXT): Must be "MCQ5" for multiple choice
- `is_active` (BOOLEAN): Must be true

### Expected options format:
```json
{
  "A": "First option text",
  "B": "Second option text",
  "C": "Third option text",
  "D": "Fourth option text",
  "E": "Fifth option text"
}
```

### Common Issues:
1. **Empty stem**: Options show but no question text
2. **Wrong options format**: Options array instead of object (shows empty options)
3. **Missing question_format**: Defaults to MCQ5 but may cause issues

## Research Pipeline (Batches 1-4)

The exam research process is critical for the system to work properly:

| Batch | Purpose | Data Generated |
|-------|---------|----------------|
| 1 | Exam Structure | Sections, timing, scoring, difficulty distribution |
| 2 | Content Taxonomy | Topics, subtopics, skills per section |
| 3 | Construct Profiling | Construct weights, cognitive levels, time expectations per node |
| 4 | Error Patterns | Exam-specific error tags, detection signals, prevalence |

### After Research Completes:
- Error tags are automatically created in the `tags` table
- Taxonomy nodes get default construct weights
- Questions can be generated with proper metadata

## Error Tags

Error tags are used to track common mistakes students make.

### Research-Generated Tags (Recommended)
After running Batch 4, exam-specific tags like these are created:
- `ERR.UTBK.SIGN_ERROR` - Sign mistakes in calculations
- `ERR.UTBK.MISREAD_NEGATION` - Missing KECUALI/NOT in question
- etc.

### Fallback Tags
If research hasn't been run, use `scripts/seed-error-tags.sql` to create generic tags:
- `ERR.RUSHED` - Too fast
- `ERR.SLOW` - Too slow
- `ERR.CARELESS` - Fast but wrong
- `ERR.STRUGGLE` - Slow and wrong

## Scores Showing 50

All construct scores showing 50 is **expected behavior** for new users. The score of 50 represents:
- Default starting value for all constructs
- Updated as the user completes more assessments

The construct weights (`teliti`, `speed`, `reasoning`, `computation`, `reading`) are computed from:
1. User's attempts and correctness
2. Time spent vs expected time
3. Question difficulty

To see real scores, users need to:
1. Complete baseline assessments
2. Have questions with proper `construct_weights` set (generated by research)

## Question Generation via Admin

When generating questions via the admin interface, ensure:

1. **AI generates proper format**: The question generator should output:
   ```json
   {
     "stem": "Full question text here",
     "options": {"A": "...", "B": "...", "C": "...", "D": "...", "E": "..."},
     "correct_answer": "A",
     "question_format": "MCQ5"
   }
   ```

2. **construct_weights are set**: Each question needs weights for accurate scoring:
   ```json
   {
     "teliti": 0.2,
     "speed": 0.2,
     "reasoning": 0.2,
     "computation": 0.2,
     "reading": 0.2
   }
   ```

## Verification

After running all scripts, verify with:

```sql
-- Check questions are properly formatted
SELECT id, stem, options, correct_answer, question_format
FROM questions
WHERE is_active = true
LIMIT 5;

-- Check error tags exist
SELECT * FROM tags WHERE category = 'error';

-- Check analytics_snapshots has all columns
SELECT column_name FROM information_schema.columns
WHERE table_name = 'analytics_snapshots';
```
